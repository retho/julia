{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA\n",
    "\n",
    "- https://cuda.juliagpu.org/stable/tutorials/introduction/\n",
    "- https://enccs.github.io/Julia-for-HPC/guide/\n",
    "\n",
    "### pde tutorials\n",
    "- https://pde-on-gpu.vaw.ethz.ch/lecture1/\n",
    "- https://pde-on-gpu.vaw.ethz.ch/lecture3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, (x = 1024, y = 1024, z = 64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import CUDA as cu\n",
    "import CUDA: @cushow, @cuda, CuArray\n",
    "import BenchmarkTools: @btime\n",
    "import Test as t\n",
    "import .Threads: @threads\n",
    "\n",
    "@assert CUDA.functional()\n",
    "# CUDA.versioninfo()\n",
    "\n",
    "# * https://discourse.julialang.org/t/cuda-threads-and-blocks-confusion/54816\n",
    "cudimmax = cu.attribute(cu.device(),cu.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "cudims = (;\n",
    "  x=cu.attribute(cu.device(), cu.DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X),\n",
    "  y=cu.attribute(cu.device(), cu.DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y),\n",
    "  z=cu.attribute(cu.device(), cu.DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z)\n",
    ")\n",
    "(cudimmax, cudims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_grid_dim_z (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * gpu utils\n",
    "\n",
    "function gpu_grid_idx_x()\n",
    "  block_dim = cu.blockDim().x\n",
    "\n",
    "  thread_idx = cu.threadIdx().x\n",
    "  block_idx = cu.blockIdx().x\n",
    "  grid_idx = (block_idx - 1) * block_dim + thread_idx;\n",
    "\n",
    "  return grid_idx\n",
    "end\n",
    "function gpu_grid_dim_x()\n",
    "  grid_dim = cu.gridDim().x * cu.blockDim().x\n",
    "  return grid_dim\n",
    "end\n",
    "\n",
    "function gpu_grid_idx_y()\n",
    "  block_dim = cu.blockDim().y\n",
    "\n",
    "  thread_idx = cu.threadIdx().y\n",
    "  block_idx = cu.blockIdx().y\n",
    "  grid_idx = (block_idx - 1) * block_dim + thread_idx;\n",
    "\n",
    "  return grid_idx\n",
    "end\n",
    "function gpu_grid_dim_y()\n",
    "  grid_dim = cu.gridDim().y * cu.blockDim().y\n",
    "  return grid_dim\n",
    "end\n",
    "\n",
    "function gpu_grid_idx_z()\n",
    "  block_dim = cu.blockDim().z\n",
    "\n",
    "  thread_idx = cu.threadIdx().z\n",
    "  block_idx = cu.blockIdx().z\n",
    "  grid_idx = (block_idx - 1) * block_dim + thread_idx;\n",
    "\n",
    "  return grid_idx\n",
    "end\n",
    "function gpu_grid_dim_z()\n",
    "  grid_dim = cu.gridDim().z * cu.blockDim().z\n",
    "  return grid_dim\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(grid_idx, grid_dim) = (1, 6)\n",
      "(grid_idx, grid_dim) = (2, 6)\n",
      "(grid_idx, grid_dim) = (3, 6)\n",
      "(grid_idx, grid_dim) = (4, 6)\n",
      "(grid_idx, grid_dim) = (5, 6)\n",
      "(grid_idx, grid_dim) = (6, 6)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "  gpu_log = () -> begin\n",
    "    grid_idx = gpu_grid_idx_x()\n",
    "    grid_dim = gpu_grid_dim_x()\n",
    "    @cushow (grid_idx, grid_dim)\n",
    "    return\n",
    "  end\n",
    "  @cuda threads=3 blocks=2 gpu_log()\n",
    "  cu.synchronize()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(grid_idx_x, grid_idx_y) = (3, 4)\n",
      "(grid_idx_x, grid_idx_y) = (4, 4)\n",
      "(grid_idx_x, grid_idx_y) = (3, 5)\n",
      "(grid_idx_x, grid_idx_y) = (4, 5)\n",
      "(grid_idx_x, grid_idx_y) = (3, 6)\n",
      "(grid_idx_x, grid_idx_y) = (4, 6)\n",
      "(grid_idx_x, grid_idx_y) = (1, 1)\n",
      "(grid_idx_x, grid_idx_y) = (2, 1)\n",
      "(grid_idx_x, grid_idx_y) = (1, 2)\n",
      "(grid_idx_x, grid_idx_y) = (2, 2)\n",
      "(grid_idx_x, grid_idx_y) = (1, 3)\n",
      "(grid_idx_x, grid_idx_y) = (2, 3)\n",
      "(grid_idx_x, grid_idx_y) = (1, 4)\n",
      "(grid_idx_x, grid_idx_y) = (2, 4)\n",
      "(grid_idx_x, grid_idx_y) = (1, 5)\n",
      "(grid_idx_x, grid_idx_y) = (2, 5)\n",
      "(grid_idx_x, grid_idx_y) = (1, 6)\n",
      "(grid_idx_x, grid_idx_y) = (2, 6)\n",
      "(grid_idx_x, grid_idx_y) = (3, 1)\n",
      "(grid_idx_x, grid_idx_y) = (4, 1)\n",
      "(grid_idx_x, grid_idx_y) = (3, 2)\n",
      "(grid_idx_x, grid_idx_y) = (4, 2)\n",
      "(grid_idx_x, grid_idx_y) = (3, 3)\n",
      "(grid_idx_x, grid_idx_y) = (4, 3)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "  gpu_log_idx = () -> begin\n",
    "    grid_idx_x = gpu_grid_idx_x()\n",
    "    grid_idx_y = gpu_grid_idx_y()\n",
    "    @cushow (grid_idx_x, grid_idx_y)\n",
    "    return\n",
    "  end\n",
    "  @cuda threads = (2, 3) blocks = (2, 2) gpu_log_idx()\n",
    "  cu.synchronize()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  119.850 ms (51 allocations: 5.27 KiB)\n"
     ]
    }
   ],
   "source": [
    "const N = 2^27\n",
    "x = zeros(Float32, N)\n",
    "y = zeros(Float32, N)\n",
    "\n",
    "fill!(x, 1)\n",
    "fill!(y, 2)\n",
    "\n",
    "bench_cpu_add! = let\n",
    "  function cpu_add!(u, v)\n",
    "    @threads for i in 1:N\n",
    "      @inbounds u[i] = u[i] + v[i]\n",
    "    end\n",
    "    return\n",
    "  end\n",
    "\n",
    "  function bench_cpu_add!(u, v)\n",
    "    cpu_add!(u, v)\n",
    "  end\n",
    "\n",
    "  bench_cpu_add!(x, y)\n",
    "  t.@test t.all(x .== 3)\n",
    "\n",
    "  bench_cpu_add!\n",
    "end\n",
    "\n",
    "fill!(x, 1)\n",
    "@btime bench_cpu_add!($x, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(threads, blocks) = (1, 1)\n",
      "  9.497 s (366 allocations: 12.92 KiB)\n",
      "(threads, blocks) = (128, 1)\n",
      "  336.437 ms (89 allocations: 4.91 KiB)\n",
      "(threads, blocks) = (256, 1)\n",
      "  178.386 ms (83 allocations: 4.78 KiB)\n",
      "(threads, blocks) = (128, 2)\n",
      "  178.280 ms (82 allocations: 4.75 KiB)\n",
      "(threads, blocks) = (256, 2)\n",
      "  98.386 ms (80 allocations: 4.72 KiB)\n",
      "(threads, blocks) = (1024, 1)\n",
      "  61.597 ms (80 allocations: 4.70 KiB)\n",
      "(threads, blocks) = (1024, 2)\n",
      "  42.442 ms (80 allocations: 4.70 KiB)\n",
      "(threads, blocks) = (1024, 14)\n",
      "  37.092 ms (80 allocations: 4.70 KiB)\n",
      "(threads, blocks) = (128, 1048576)\n",
      "  33.837 ms (80 allocations: 4.70 KiB)\n",
      "(threads, blocks) = (256, 524288)\n",
      "  34.093 ms (80 allocations: 4.70 KiB)\n",
      "(threads, blocks) = (1024, 131072)\n",
      "  34.087 ms (81 allocations: 4.72 KiB)\n"
     ]
    }
   ],
   "source": [
    "const N = 2^27\n",
    "x = cu.fill(1.0f0, N)\n",
    "y = cu.fill(2.0f0, N)\n",
    "\n",
    "bench_gpu_add! = let\n",
    "  function gpu_add!(u, v)\n",
    "    ix = gpu_grid_idx_x()\n",
    "    dim = gpu_grid_dim_x()\n",
    "    for i in ix:dim:N\n",
    "      @inbounds u[i] += v[i]\n",
    "    end\n",
    "    return nothing\n",
    "  end\n",
    "\n",
    "  function bench_gpu_add!(u, v; threads, blocks)\n",
    "    @cuda threads = threads blocks = blocks gpu_add!(x, y)\n",
    "    cu.synchronize()\n",
    "  end\n",
    "\n",
    "  bench_gpu_add!(x, y; threads=1, blocks=1)\n",
    "  t.@test all(Array(x) .== 3.0f0)\n",
    "\n",
    "  bench_gpu_add!\n",
    "end\n",
    "\n",
    "function bench(threads, blocks)\n",
    "  @show (threads, blocks)\n",
    "  cu.fill!(x, 1)\n",
    "  @btime bench_gpu_add!($x, $y; threads=$threads, blocks=$blocks)\n",
    "end\n",
    "\n",
    "bench(1, 1)\n",
    "bench(128, 1)\n",
    "bench(256, 1)\n",
    "bench(128, 2)\n",
    "bench(256, 2)\n",
    "bench(cudims.x, 1)\n",
    "bench(cudims.x, 2)\n",
    "bench(cudims.x, 14)\n",
    "bench(128, ceil(Int, N/128))\n",
    "bench(256, ceil(Int, N/256))\n",
    "bench(cudims.x, ceil(Int, N/cudims.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  821.279 ms (51 allocations: 5.27 KiB)\n"
     ]
    }
   ],
   "source": [
    "const M = 2^13\n",
    "x = zeros(Float32, M, M)\n",
    "y = zeros(Float32, M, M)\n",
    "\n",
    "fill!(x, 1)\n",
    "fill!(y, 2)\n",
    "\n",
    "bench_cpu_add! = let\n",
    "  function cpu_add!(u, v)\n",
    "    @threads for i in 1:M\n",
    "      for j in 1:M\n",
    "        @inbounds u[i, j] = u[i, j] + v[i, j]\n",
    "      end\n",
    "    end\n",
    "    return\n",
    "  end\n",
    "\n",
    "  function bench_cpu_add!(u, v)\n",
    "    cpu_add!(u, v)\n",
    "  end\n",
    "\n",
    "  bench_cpu_add!(x, y)\n",
    "  t.@test t.all(x .== 3)\n",
    "\n",
    "  bench_cpu_add!\n",
    "end\n",
    "\n",
    "fill!(x, 1)\n",
    "@btime bench_cpu_add!($x, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(threads, blocks) = (1, 1)\n",
      "  21.133 s (723 allocations: 23.05 KiB)\n",
      "(threads, blocks) = (32, 10)\n",
      "  18.791 ms (81 allocations: 4.88 KiB)\n"
     ]
    }
   ],
   "source": [
    "const M = 2^13\n",
    "x = cu.fill(1.0f0, M, M)\n",
    "y = cu.fill(2.0f0, M, M)\n",
    "\n",
    "bench_gpu_add! = let\n",
    "  function gpu_add!(u, v)\n",
    "    ix_x = gpu_grid_idx_x()\n",
    "    dim_x = gpu_grid_dim_x()\n",
    "    ix_y = gpu_grid_idx_y()\n",
    "    dim_y = gpu_grid_dim_y()\n",
    "    for i in ix_x:dim_x:M\n",
    "      for j in ix_y:dim_y:M\n",
    "        @inbounds u[i, j] += v[i, j]\n",
    "      end\n",
    "    end\n",
    "    return nothing\n",
    "  end\n",
    "\n",
    "  function bench_gpu_add!(u, v; threads, blocks)\n",
    "    @cuda threads = (threads, threads) blocks = (blocks, blocks) gpu_add!(x, y)\n",
    "    cu.synchronize()\n",
    "  end\n",
    "\n",
    "  bench_gpu_add!(x, y; threads=1, blocks=1)\n",
    "  t.@test all(Array(x) .== 3.0f0)\n",
    "\n",
    "  bench_gpu_add!\n",
    "end\n",
    "\n",
    "function bench(threads, blocks)\n",
    "  @show (threads, blocks)\n",
    "  cu.fill!(x, 1)\n",
    "  @btime bench_gpu_add!($x, $y; threads=$threads, blocks=$blocks)\n",
    "end\n",
    "\n",
    "bench(1, 1)\n",
    "bench(floor(Int, sqrt(cudims.x)), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (sysimage, threads) 1.8.4",
   "language": "julia",
   "name": "julia-_sysimage_-threads_-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
